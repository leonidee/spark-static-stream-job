version: '3.8'

networks:
  internal:
    driver: bridge

x-spark-common: &spark-common
  build: 
    dockerfile: docker/spark/Dockerfile
  environment: 
    - SPARK_MODE=worker
    - SPARK_MASTER_URL=spark://spark-master:7077
    - SPARK_WORKER_MEMORY=2G
    - SPARK_WORKER_CORES=1
    - SPARK_RPC_AUTHENTICATION_ENABLED=no
    - SPARK_RPC_ENCRYPTION_ENABLED=no
    - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
    - SPARK_SSL_ENABLED=no
    - SPARK_USER=spark
    # More environment variables natively supported by Apache Spark
    # can be found here -> https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts
  volumes:
    - ./src:/app/src
    - ./jobs:/app/jobs
    - ./spark:/app/spark
    - ./.env:/app/.env
    - ./config.yaml:/app/config.yaml
    - ./pyproject.toml:/app/pyproject.toml
    - ./poetry.lock:/app/poetry.lock
    - ./config/spark/log4j2.properties:/opt/bitnami/spark/conf/log4j2.properties
  restart: unless-stopped
  networks:
    - internal

x-producer-common: &producer-common
  build: 
    dockerfile: docker/producer/Dockerfile
  depends_on:
    - kafka
  command: /bin/bash /app/producer/run.sh
  restart: on-failure
  networks:
    - internal

services:
  spark-master:
    <<: *spark-common
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - "8080:8080"
      - "7077:7077"

  spark-worker-1:
    <<: *spark-common
    container_name: spark-worker-1
    ports:
      - "8081:8081"
    depends_on:
      - spark-master

  spark-worker-2:
    <<: *spark-common
    container_name: spark-worker-2
    ports:
      - "8082:8081"
    depends_on:
      - spark-master

  spark-worker-3:
    <<: *spark-common
    container_name: spark-worker-3
    ports:
      - "8083:8081"
    depends_on:
      - spark-master

  spark-worker-4:
    <<: *spark-common
    container_name: spark-worker-4
    ports:
      - "8084:8081"
    depends_on:
      - spark-master

  spark-worker-5:
    <<: *spark-common
    container_name: spark-worker-5
    ports:
      - "8085:8081"
    depends_on:
      - spark-master

# https://github.com/bitnami/containers/tree/main/bitnami/kafka
  kafka:
    image: docker.io/bitnami/kafka:3.4
    container_name: kafka
    ports:
      - 9092:9092
    environment:
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_BROKER_ID=1
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://84.201.165.117:9092
      - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=1
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_LOG_RETENTION_HOURS=24
    volumes:
      - ./config/kafka/log4j.properties:/opt/bitnami/kafka/config/log4j.properties 
    restart: unless-stopped
    networks:
      - internal

  adv-campaign-producer:
    <<: *producer-common
    container_name: adv-campaign-producer
    volumes:
      - ./src:/app/src
      - ./.env:/app/.env
      - ./producer/adv-campaign:/app/producer
      - ./config.yaml:/app/config.yaml

  clients-locations-producer:
    <<: *producer-common
    container_name: clients-locations-producer
    volumes:
      - ./src:/app/src
      - ./.env:/app/.env
      - ./producer/clients-locations:/app/producer
      - ./config.yaml:/app/config.yaml


    


